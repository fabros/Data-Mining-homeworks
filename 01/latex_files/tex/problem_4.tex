% !TeX spellcheck = en_GB

\section{Problem 4}

In what follows, we present the development process of our recipes search engine. We will focus on techniques and technologies we used to implement it, giving to the reader some brief explanations of key Information Retrieval (IR) concepts. You can find the source code in the material attached with this documentation.\\\\
\textbf{Disclaimer}: to run our code, be sure to be on a Linux machine (or any other that allows you to run Bash scripts) and have Python 2.7 installed. 


\subsection{Recipes gathering}

Our initial approach was to implement a Python crawler to gather all the 11300 recipes that BBC site\cite{bbc} provides. We decided to use \textit{urllib2}\cite{urllib2} module to download .html pages containing recipes info and \textit{BeautifulSoup}\cite{beaut_soup} module to parse them and get links to other recipes.\\
Even though we succeeded in building a quite good crawler, we soon realized that this approach gave us no guarantees of downloading all recipes in the site and that it would take a lot of time to check which were the missing ones. Therefore, we looked for a better approach and found out that BBC exposes an .xml sitemap that gathers all the links to web pages of the site. At that point we were able to extract from that file all the relevant links, using \textit{BeautifulSoup}\cite{beaut_soup} utilities and regexes. You can find the code we used for this purpose in \textit{list\_recipes.py}.\\
The Python script we used was made to write recipes links to a .txt file. Then, we wrote a simple Bash script (\textit{download\_recipes.sh}) to process this file and download the recipes using \textit{wget} command. The real strength of this approach was that we were able to decide whether all recipes had been downloaded or not (due, for instance, to network issues) by checking downloaded files against the list of recipes urls (see \textit{check\_recipes.sh}).\\
To run our scripts, open a terminal and type
\begin{lstlisting}
	$ python list_recipes.py
	$ ./download_recipes.sh
\end{lstlisting}
Note that the entire download process could take more or less 4 hours to terminate. To check the result, type
\begin{lstlisting}
	$ ./check_recipes.sh
\end{lstlisting}
In case any recipe was not downloaded, the script will output the list of related urls and you can restart the download process (only for missing ones).


\subsection{Pre-processing}

In order to pre-process recipes, we first needed to extract from downloaded .html pages relevant informations.  Even in this case, we used \textit{BeautifulSoup}\cite{beaut_soup} utilities to parse them. Then, we stored obtained info in a big .tsv file, where each row represented a recipe, using \textit{unicodecsv}\cite{csv} module. You can find the code we used for this purpose in \textit{store\_recipes.py}. To run the script, open a terminal and type
\begin{lstlisting}
	$ python store_recipes.py
\end{lstlisting}
Note that the process could take more or less 15 minutes to terminate. Resulting file (\textit{recipes.tsv}) should appear like in the picture below.
\begin{center}
	\vspace{5mm}
	\includegraphics[scale=0.3]{img/recipes-tsv.jpg}
\end{center}
On that result, we applied pre-processing techniques using \textit{NLTK}\cite{nltk} library. We created a separated Python module (named \textit{preprocessing.py}) encapsulating all pre-processing logic, to be able to reuse it also in query processing part. In particular, this module includes functions implementing:
\begin{itemize}
	\item Tokenization
	\item Normalization
	\begin{itemize}
		\item remove diacritics.
		\item remove non-alphanumeric characters.
		\item convert text to lowercase.
		\item convert special unicode characters (e.g. fractions) into a somewhat "equivalent" ASCII representation.
	\end{itemize}
	\item Stopwords removal
	\item Stemming
\end{itemize}
Then, we made another script to store pre-processing result into another big .tsv file. You can find the code we used for this purpose in \textit{preprocess\_recipes.py}. To run the script, open a terminal and type
\begin{lstlisting}
	$ python preprocess_recipes.py
\end{lstlisting}
Resulting file (\textit{recipes-prep.tsv}) should appear like in the picture below.
\begin{center}
	\vspace{5mm}
	\includegraphics[scale=0.3]{img/recipes-prep-tsv.jpg}
\end{center}


\subsection{Inverted index building}

An Inverted Index\cite{inv_ind} is a particular data structure storing a mapping from words in a corpus of documents to their locations (i.e. in which documents they occur). We needed such that structure to perform proximity queries on our recipes corpus, using cosine-similarity measure. You can find the code we used for this purpose in \textit{build\_index.py}.\\
Basically, we developed a script that scans the whole corpus, building as you go both the doc-term (sparse) matrix\cite{doc_term} and its transpose, i.e. the index. The index is represented as a Python dictionary, in which an entry has following structure:
\begin{center}
	\textit{(term, list of postings pairs)}
\end{center}
A \textit{posting pair} is represented as a 2-items Python list \textit{(doc\_id, tf)}, where:
\begin{itemize}
	\item \textit{doc\_id} is the document id.
	\item \textit{tf} is the term frequency (i.e. number of times term occurs) in \textit{doc\_id}.
\end{itemize}
Note that the list is sorted with respect to \textit{doc\_id}.\\
The doc-term matrix is represented as a Python list of documents (i.e. the rows). Each row is a sparse vector, implemented with a Python dictionary in which an entry has following structure:
\begin{center}
	\textit{(term, tf)}
\end{center}
For the sake of memory efficiency, we made the two data structures share their entries. Indeed, matrix cells (instead of the single tf value) actually contain (a reference to) the corresponding posting in the index. Therefore, the script scans the documents in the corpus, updating as you go the value of \textit{tf} of the terms it encounters.\\
Note that we needed doc-term matrix to perform an optimization of the index for query processing: for each posting in a term posting list, we want to store the complete query-independent factor, i.e.
\begin{align*}
	tf_{term,doc} * \dfrac{idf_{term}^2}{||doc||}
\end{align*}
where $idf_{term}$ is the Inverse Document Frequency\cite{tf_idf} of the term, and $||doc||$ is the length of the vector representing the document.\\
To run the script, open a terminal and type
\begin{lstlisting}
	$ python build_index.py
\end{lstlisting}
The result a dump of both the doc-term matrix and the index to separated .pickle files (we used \textit{pickle}\cite{pickle} module to do that). This allows to easily recreate the index in memory when needed for query processing. 


\subsection{Query processing}




\subsection{Extra features}

















